{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating database for searcher\n",
    "import pandas as pd\n",
    "data = pd.read_csv('Russian_poems.csv', sep='\\t')\n",
    "data = data['text']\n",
    "docam = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for converting texts from document\n",
    "import pymorphy2, re\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def normy_text(text):\n",
    "    new_text = ''\n",
    "    text = re.sub('[^а-яА-Яa-zA-ZёЁ]', ' ', text.lower())\n",
    "    words = text.replace('  ', ' ').split()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        new_text += ' ' + p.normal_form\n",
    "    new_text = new_text[1:]\n",
    "    return new_text\n",
    "def normy_text_lite(text):\n",
    "    text = re.sub('[^а-яА-Яa-zA-ZёЁ]', ' ', text.lower())\n",
    "    words = text.replace('  ', ' ').split()\n",
    "    i=0\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        words[i] = p.normal_form\n",
    "        i+=1\n",
    "    return words\n",
    "def normy_word(word):\n",
    "    p = morph.parse(word)[0]\n",
    "    word = p.normal_form\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а2 - data frame of all words in all documents with their IDF\n",
    "a2 = pd.DataFrame(columns=['word', 'idf'])\n",
    "for doc in data:\n",
    "    a1 = normy_text_lite(doc)\n",
    "    atemp = pd.DataFrame(columns=['word', 'idf'])\n",
    "    atemp['word'] = a1\n",
    "    atemp = atemp.drop_duplicates().reset_index().drop(['index'], axis = 1)\n",
    "    a2 = a2.append(atemp, ignore_index=True)\n",
    "    a2 = a2.drop_duplicates().reset_index().drop(['index'], axis = 1)\n",
    "a2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#normalize texts\n",
    "datanorm = data.apply(lambda x: normy_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating IDF\n",
    "def count_idf ():\n",
    "    i = 0\n",
    "    for word in a2['word']:\n",
    "        docamw = 0\n",
    "        for doc in datanorm:\n",
    "            if (re.search(word, doc)!=None): \n",
    "                docamw+=1\n",
    "        if (docamw == 0): \n",
    "            idft = 0\n",
    "        else: \n",
    "            idft = math.log(docam/docamw)\n",
    "        a2.loc[[i], ['idf']] = idft\n",
    "        i+=1\n",
    "count_idf()\n",
    "a2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating TF\n",
    "def findam (word, doc): #function dividing a document into words and allowing you to compare and display the number of words in a document\n",
    "    am=0\n",
    "    words = doc.split()\n",
    "    for word1 in words:\n",
    "        if(word ==word1):\n",
    "            am+=1\n",
    "    return am\n",
    "def cout_tf_idf(data, datanorm, tf_idf):\n",
    "    d1 = pd.DataFrame(columns=['doc', 'doc_lem', 'tf_idf'])\n",
    "    d1['doc'] = data\n",
    "    d1['tf_idf'] = tf_idf\n",
    "    d1['doc_lem'] = datanorm\n",
    "    for word in SWords:\n",
    "#          normalize the word from the search\n",
    "        word = normy_word(word)\n",
    "#         print(word)\n",
    "#         looking for a word in the data frame\n",
    "        idf1 = a2[a2['word']==word]['idf'].to_list() #takes idf from data frame\n",
    "        if (len(idf1)==0):\n",
    "            continue\n",
    "        else: idf1 = idf1[0]\n",
    "        i=0\n",
    "#         print(idf1)\n",
    "        if (idf1==0): continue\n",
    "        else:\n",
    "            for doc in datanorm:\n",
    "#         looking for the amount of this word in the document\n",
    "                wordam = findam(word, doc)\n",
    "#         print(wordam)\n",
    "#         length of the entire document\n",
    "                wordallam = len(doc)\n",
    "#         find tf\n",
    "                tf = wordam/wordallam \n",
    "#         find tf*idf\n",
    "                tf_idf[i]=tf*idf1;\n",
    "                i+=1\n",
    "#     write to the data frame, adding tf*idf\n",
    "        d1['tf_idf'] = d1['tf_idf'] + tf_idf\n",
    "#     sort descending tf*idf after calculating tf*idf for all words and documents\n",
    "    d1 = d1.sort_values(by=['tf_idf'], ascending=False).reset_index().drop(['index'], axis = 1)\n",
    "    return d1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Search = input()\n",
    "SWords = Search.lower().split() #делит поисковый запрос на слова\n",
    "for word in SWords:\n",
    "    print (word)\n",
    "tf_idf = [0.0]*len(data)\n",
    "d1 = cout_tf_idf(data, datanorm, tf_idf)\n",
    "d1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display relevant texts with the most suitable parts, which we also find through tf_idf\n",
    "i=0\n",
    "for doc in d1['doc']:\n",
    "    if (d1['tf_idf'][i]>0):\n",
    "        sentanses = doc.split('.')\n",
    "        sentnorm = ['']*len(sentanses)\n",
    "        j=0\n",
    "        for sentanse in sentanses:\n",
    "            if (len(sentanse) == 0):\n",
    "                sentanse = 'empty'\n",
    "            sentnorm[j] = normy_text(sentanse) #normalize sentences\n",
    "#             print(sentnorm[j])\n",
    "            j+=1\n",
    "        tf_idf_sent = [0.0]*len(sentanses)\n",
    "#         print(len(sentanses))\n",
    "        d2 = cout_tf_idf(sentanses, sentnorm, tf_idf_sent)\n",
    "        print('В ' + str(i+1) + ' документе')\n",
    "        print(d2['doc'][0])\n",
    "        print('\\n')\n",
    "    else: continue\n",
    "    i+=1\n",
    "    if (i==5): break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import _levenshtein\n",
    "def cout_dist(data, dist_sum):\n",
    "    d3 = pd.DataFrame(columns=['doc', 'dist'])\n",
    "    d3['doc'] = data\n",
    "    d3['dist'] = dist_sum\n",
    "    for word in SWords:\n",
    "#          normalize the word from the search\n",
    "        word1 = normy_word(word)\n",
    "#         print(word1)\n",
    "#         looking for a word in the data frame\n",
    "        idf1 = a2[a2['word']==word1]['idf'].to_list() #take idf\n",
    "        if (len(idf1)==0): continue\n",
    "        else: idf1 = idf1[0]\n",
    "        if (idf1==0): continue #if idf = 0, ignore\n",
    "        else:\n",
    "            i=0\n",
    "            for doc in data:\n",
    "                doc = re.sub('[^а-яА-Яa-zA-ZёЁ]', ' ', doc.lower())\n",
    "                words = doc.replace('  ', ' ').split()\n",
    "                if (len(words) == 0) : continue\n",
    "                else:\n",
    "                    min_dist = _levenshtein.distance(word, words[0])\n",
    "                for worddoc in words:\n",
    "                    min_dist_temp = _levenshtein.distance(word, worddoc)\n",
    "                    if (min_dist_temp<min_dist):\n",
    "                        min_dist = min_dist_temp\n",
    "                dist_sum[i] = min_dist\n",
    "                i+=1\n",
    "#     write to the data frame by adding dist_sum\n",
    "            d3['dist'] = d3['dist'] + dist_sum\n",
    "#     sort ascending dist_sum after calculating dist_sum for all words and documents\n",
    "    d3 = d3.sort_values(by=['dist']).reset_index().drop(['index'], axis = 1)\n",
    "    return d3\n",
    "dist_sum = [0.0]*len(data)\n",
    "d3 = cout_dist(data, dist_sum)\n",
    "d3.head(5)\n",
    "i=0\n",
    "for doc in d3['doc']:\n",
    "    sentanses = doc.split('.')\n",
    "    for sentanse in sentanses:\n",
    "        if (len(sentanse) == 0):\n",
    "            sentanse = 'empty'\n",
    "    dist_sum_sent = [0.0]*len(sentanses)\n",
    "    d4 = cout_dist(sentanses, dist_sum_sent)\n",
    "    print('В ' + str(i+1) + ' документе')\n",
    "    print(d4['doc'][0])\n",
    "    print(d4['doc'][1])\n",
    "    print('\\n')\n",
    "    i+=1\n",
    "    if (i==5): break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
